{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import turicreate as tc\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>Materializing SFrame...</pre>"
      ],
      "text/plain": [
       "Materializing SFrame..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Done.</pre>"
      ],
      "text/plain": [
       "Done."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the data\n",
    "data =  tc.SFrame('ig02_plates.sframe')\n",
    "\n",
    "data['image_with_ground_truth'] = \\\n",
    "    tc.object_detector.util.draw_bounding_boxes(data['image'], data['annotations'])\n",
    "data.explore()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a train-test split\n",
    "train_data, test_data = data.random_split(0.8)\n",
    "\n",
    "# Save the training and test sets\n",
    "train_data.save('train_data_split.sframe')\n",
    "test_data.save('test_data_split.sframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 'annotations' as annotations column\n",
      "2018-11-23 14:48:07  Training    1/1000  Loss  4.275\n",
      "2018-11-23 14:48:29  Training    2/1000  Loss  4.282\n",
      "2018-11-23 14:48:50  Training    3/1000  Loss  4.287\n",
      "2018-11-23 14:49:07  Training    4/1000  Loss  4.301\n",
      "2018-11-23 14:49:32  Training    5/1000  Loss  4.305\n",
      "2018-11-23 14:49:53  Training    6/1000  Loss  4.295\n",
      "2018-11-23 14:50:14  Training    7/1000  Loss  4.306\n",
      "2018-11-23 14:50:33  Training    8/1000  Loss  4.273\n",
      "2018-11-23 14:50:51  Training    9/1000  Loss  4.282\n",
      "2018-11-23 14:51:10  Training   10/1000  Loss  4.311\n",
      "2018-11-23 14:51:29  Training   11/1000  Loss  4.279\n",
      "2018-11-23 14:51:50  Training   12/1000  Loss  4.249\n",
      "2018-11-23 14:52:09  Training   13/1000  Loss  4.264\n",
      "2018-11-23 14:52:27  Training   14/1000  Loss  4.229\n",
      "2018-11-23 14:52:46  Training   15/1000  Loss  4.177\n",
      "2018-11-23 14:53:04  Training   16/1000  Loss  4.197\n",
      "2018-11-23 14:53:18  Training   17/1000  Loss  4.203\n",
      "2018-11-23 14:53:37  Training   18/1000  Loss  4.177\n",
      "2018-11-23 14:53:55  Training   19/1000  Loss  4.119\n",
      "2018-11-23 14:54:12  Training   20/1000  Loss  4.096\n",
      "2018-11-23 14:54:34  Training   21/1000  Loss  4.072\n",
      "2018-11-23 14:54:55  Training   22/1000  Loss  4.034\n",
      "2018-11-23 14:55:14  Training   23/1000  Loss  3.952\n",
      "2018-11-23 14:55:35  Training   24/1000  Loss  3.936\n",
      "2018-11-23 14:55:53  Training   25/1000  Loss  3.901\n",
      "2018-11-23 14:56:12  Training   26/1000  Loss  3.871\n",
      "2018-11-23 14:56:28  Training   27/1000  Loss  3.829\n",
      "2018-11-23 14:56:48  Training   28/1000  Loss  3.780\n",
      "2018-11-23 14:57:03  Training   29/1000  Loss  3.711\n",
      "2018-11-23 14:57:20  Training   30/1000  Loss  3.647\n",
      "2018-11-23 14:57:39  Training   31/1000  Loss  3.618\n",
      "2018-11-23 14:57:59  Training   32/1000  Loss  3.551\n",
      "2018-11-23 14:58:16  Training   33/1000  Loss  3.490\n",
      "2018-11-23 14:58:31  Training   34/1000  Loss  3.423\n",
      "2018-11-23 14:58:49  Training   35/1000  Loss  3.380\n",
      "2018-11-23 14:59:04  Training   36/1000  Loss  3.346\n",
      "2018-11-23 14:59:28  Training   37/1000  Loss  3.322\n",
      "2018-11-23 14:59:48  Training   38/1000  Loss  3.257\n",
      "2018-11-23 15:00:07  Training   39/1000  Loss  3.239\n",
      "2018-11-23 15:00:24  Training   40/1000  Loss  3.170\n",
      "2018-11-23 15:00:43  Training   41/1000  Loss  3.088\n",
      "2018-11-23 15:01:14  Training   42/1000  Loss  3.027\n",
      "2018-11-23 15:02:06  Training   43/1000  Loss  2.970\n",
      "2018-11-23 15:02:29  Training   44/1000  Loss  2.913\n",
      "2018-11-23 15:02:50  Training   45/1000  Loss  2.863\n",
      "2018-11-23 15:03:10  Training   46/1000  Loss  2.788\n",
      "2018-11-23 15:03:35  Training   47/1000  Loss  2.758\n",
      "2018-11-23 15:04:04  Training   48/1000  Loss  2.721\n",
      "2018-11-23 15:04:33  Training   49/1000  Loss  2.699\n",
      "2018-11-23 15:04:54  Training   50/1000  Loss  2.683\n",
      "2018-11-23 15:05:20  Training   51/1000  Loss  2.643\n",
      "2018-11-23 15:05:46  Training   52/1000  Loss  2.597\n",
      "2018-11-23 15:06:04  Training   53/1000  Loss  2.569\n",
      "2018-11-23 15:06:19  Training   54/1000  Loss  2.547\n",
      "2018-11-23 15:06:37  Training   55/1000  Loss  2.539\n",
      "2018-11-23 15:06:56  Training   56/1000  Loss  2.508\n",
      "2018-11-23 15:07:11  Training   57/1000  Loss  2.480\n",
      "2018-11-23 15:07:29  Training   58/1000  Loss  2.463\n",
      "2018-11-23 15:07:44  Training   59/1000  Loss  2.451\n",
      "2018-11-23 15:08:01  Training   60/1000  Loss  2.483\n",
      "2018-11-23 15:08:20  Training   61/1000  Loss  2.478\n",
      "2018-11-23 15:08:39  Training   62/1000  Loss  2.407\n",
      "2018-11-23 15:08:56  Training   63/1000  Loss  2.383\n",
      "2018-11-23 15:09:16  Training   64/1000  Loss  2.328\n",
      "2018-11-23 15:09:35  Training   65/1000  Loss  2.308\n",
      "2018-11-23 15:09:53  Training   66/1000  Loss  2.273\n",
      "2018-11-23 15:10:11  Training   67/1000  Loss  2.239\n",
      "2018-11-23 15:10:26  Training   68/1000  Loss  2.235\n",
      "2018-11-23 15:10:44  Training   69/1000  Loss  2.193\n",
      "2018-11-23 15:10:58  Training   70/1000  Loss  2.144\n",
      "2018-11-23 15:11:13  Training   71/1000  Loss  2.114\n",
      "2018-11-23 15:11:35  Training   72/1000  Loss  2.067\n",
      "2018-11-23 15:11:51  Training   73/1000  Loss  2.046\n",
      "2018-11-23 15:12:05  Training   74/1000  Loss  1.989\n",
      "2018-11-23 15:12:20  Training   75/1000  Loss  1.970\n",
      "2018-11-23 15:12:36  Training   76/1000  Loss  1.955\n",
      "2018-11-23 15:12:51  Training   77/1000  Loss  1.925\n",
      "2018-11-23 15:13:08  Training   78/1000  Loss  1.927\n",
      "2018-11-23 15:13:22  Training   79/1000  Loss  1.899\n",
      "2018-11-23 15:13:39  Training   80/1000  Loss  1.908\n",
      "2018-11-23 15:13:55  Training   81/1000  Loss  1.868\n",
      "2018-11-23 15:14:10  Training   82/1000  Loss  1.865\n",
      "2018-11-23 15:14:28  Training   83/1000  Loss  1.897\n",
      "2018-11-23 15:14:45  Training   84/1000  Loss  1.868\n",
      "2018-11-23 15:15:01  Training   85/1000  Loss  1.839\n",
      "2018-11-23 15:15:21  Training   86/1000  Loss  1.845\n",
      "2018-11-23 15:15:45  Training   87/1000  Loss  1.851\n",
      "2018-11-23 15:15:58  Training   88/1000  Loss  1.829\n",
      "2018-11-23 15:16:14  Training   89/1000  Loss  1.789\n",
      "2018-11-23 15:16:29  Training   90/1000  Loss  1.764\n",
      "2018-11-23 15:16:45  Training   91/1000  Loss  1.748\n",
      "2018-11-23 15:17:01  Training   92/1000  Loss  1.730\n",
      "2018-11-23 15:17:18  Training   93/1000  Loss  1.722\n",
      "2018-11-23 15:17:37  Training   94/1000  Loss  1.705\n",
      "2018-11-23 15:17:55  Training   95/1000  Loss  1.720\n",
      "2018-11-23 15:18:10  Training   96/1000  Loss  1.712\n",
      "2018-11-23 15:18:28  Training   97/1000  Loss  1.736\n",
      "2018-11-23 15:18:43  Training   98/1000  Loss  1.682\n",
      "2018-11-23 15:18:59  Training   99/1000  Loss  1.717\n",
      "2018-11-23 15:19:16  Training  100/1000  Loss  1.708\n",
      "2018-11-23 15:19:35  Training  101/1000  Loss  1.683\n",
      "2018-11-23 15:19:52  Training  102/1000  Loss  1.657\n",
      "2018-11-23 15:20:08  Training  103/1000  Loss  1.631\n",
      "2018-11-23 15:20:25  Training  104/1000  Loss  1.615\n",
      "2018-11-23 15:20:40  Training  105/1000  Loss  1.591\n",
      "2018-11-23 15:20:55  Training  106/1000  Loss  1.569\n",
      "2018-11-23 15:21:11  Training  107/1000  Loss  1.578\n",
      "2018-11-23 15:21:28  Training  108/1000  Loss  1.596\n",
      "2018-11-23 15:21:44  Training  109/1000  Loss  1.598\n",
      "2018-11-23 15:22:00  Training  110/1000  Loss  1.563\n",
      "2018-11-23 15:22:14  Training  111/1000  Loss  1.560\n",
      "2018-11-23 15:22:30  Training  112/1000  Loss  1.548\n",
      "2018-11-23 15:22:45  Training  113/1000  Loss  1.551\n",
      "2018-11-23 15:23:03  Training  114/1000  Loss  1.554\n",
      "2018-11-23 15:54:05  Training  115/1000  Loss  1.558\n",
      "2018-11-23 15:54:44  Training  116/1000  Loss  1.542\n",
      "2018-11-23 15:55:33  Training  117/1000  Loss  1.516\n",
      "2018-11-23 15:56:29  Training  118/1000  Loss  1.509\n",
      "2018-11-23 15:57:10  Training  119/1000  Loss  1.502\n",
      "2018-11-23 15:57:50  Training  120/1000  Loss  1.512\n",
      "2018-11-23 15:58:39  Training  121/1000  Loss  1.495\n",
      "2018-11-23 15:59:28  Training  122/1000  Loss  1.501\n",
      "2018-11-23 16:00:13  Training  123/1000  Loss  1.503\n",
      "2018-11-23 16:00:55  Training  124/1000  Loss  1.491\n",
      "2018-11-23 16:01:39  Training  125/1000  Loss  1.486\n",
      "2018-11-23 16:02:35  Training  126/1000  Loss  1.476\n",
      "2018-11-23 16:02:59  Training  127/1000  Loss  1.481\n",
      "2018-11-23 16:03:19  Training  128/1000  Loss  1.466\n",
      "2018-11-23 16:03:37  Training  129/1000  Loss  1.475\n",
      "2018-11-23 16:03:55  Training  130/1000  Loss  1.434\n",
      "2018-11-23 16:04:14  Training  131/1000  Loss  1.456\n",
      "2018-11-23 16:04:36  Training  132/1000  Loss  1.416\n",
      "2018-11-23 16:05:01  Training  133/1000  Loss  1.404\n",
      "2018-11-23 16:05:19  Training  134/1000  Loss  1.404\n",
      "2018-11-23 16:05:36  Training  135/1000  Loss  1.398\n",
      "2018-11-23 16:05:51  Training  136/1000  Loss  1.418\n",
      "2018-11-23 16:06:09  Training  137/1000  Loss  1.409\n",
      "2018-11-23 16:06:24  Training  138/1000  Loss  1.417\n",
      "2018-11-23 16:06:39  Training  139/1000  Loss  1.377\n",
      "2018-11-23 16:06:55  Training  140/1000  Loss  1.399\n",
      "2018-11-23 16:07:09  Training  141/1000  Loss  1.386\n",
      "2018-11-23 16:07:25  Training  142/1000  Loss  1.384\n",
      "2018-11-23 16:07:42  Training  143/1000  Loss  1.363\n",
      "2018-11-23 16:07:59  Training  144/1000  Loss  1.333\n",
      "2018-11-23 16:08:14  Training  145/1000  Loss  1.333\n",
      "2018-11-23 16:08:29  Training  146/1000  Loss  1.317\n",
      "2018-11-23 16:08:45  Training  147/1000  Loss  1.313\n",
      "2018-11-23 16:08:59  Training  148/1000  Loss  1.291\n",
      "2018-11-23 16:09:16  Training  149/1000  Loss  1.287\n",
      "2018-11-23 16:09:37  Training  150/1000  Loss  1.271\n",
      "2018-11-23 16:09:56  Training  151/1000  Loss  1.279\n",
      "2018-11-23 16:10:12  Training  152/1000  Loss  1.308\n",
      "2018-11-23 16:10:29  Training  153/1000  Loss  1.305\n",
      "2018-11-23 16:10:44  Training  154/1000  Loss  1.291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-23 16:10:59  Training  155/1000  Loss  1.303\n",
      "2018-11-23 16:11:20  Training  156/1000  Loss  1.299\n",
      "2018-11-23 16:11:41  Training  157/1000  Loss  1.281\n",
      "2018-11-23 16:12:01  Training  158/1000  Loss  1.267\n",
      "2018-11-23 16:12:18  Training  159/1000  Loss  1.262\n",
      "2018-11-23 16:12:33  Training  160/1000  Loss  1.250\n",
      "2018-11-23 16:12:48  Training  161/1000  Loss  1.241\n",
      "2018-11-23 16:13:04  Training  162/1000  Loss  1.218\n",
      "2018-11-23 16:13:20  Training  163/1000  Loss  1.208\n",
      "2018-11-23 16:13:37  Training  164/1000  Loss  1.236\n",
      "2018-11-23 16:13:56  Training  165/1000  Loss  1.214\n",
      "2018-11-23 16:14:11  Training  166/1000  Loss  1.232\n",
      "2018-11-23 16:14:32  Training  167/1000  Loss  1.226\n",
      "2018-11-23 16:14:48  Training  168/1000  Loss  1.226\n",
      "2018-11-23 16:15:04  Training  169/1000  Loss  1.210\n",
      "2018-11-23 16:15:21  Training  170/1000  Loss  1.204\n",
      "2018-11-23 16:15:41  Training  171/1000  Loss  1.215\n",
      "2018-11-23 16:16:02  Training  172/1000  Loss  1.234\n",
      "2018-11-23 16:16:18  Training  173/1000  Loss  1.242\n",
      "2018-11-23 16:16:37  Training  174/1000  Loss  1.239\n",
      "2018-11-23 16:16:55  Training  175/1000  Loss  1.273\n",
      "2018-11-23 16:17:12  Training  176/1000  Loss  1.263\n",
      "2018-11-23 16:17:28  Training  177/1000  Loss  1.278\n",
      "2018-11-23 16:17:44  Training  178/1000  Loss  1.296\n",
      "2018-11-23 16:18:00  Training  179/1000  Loss  1.276\n",
      "2018-11-23 16:18:16  Training  180/1000  Loss  1.263\n",
      "2018-11-23 16:18:31  Training  181/1000  Loss  1.275\n",
      "2018-11-23 16:18:46  Training  182/1000  Loss  1.244\n",
      "2018-11-23 16:19:09  Training  183/1000  Loss  1.224\n",
      "2018-11-23 16:19:27  Training  184/1000  Loss  1.214\n",
      "2018-11-23 16:19:50  Training  185/1000  Loss  1.224\n",
      "2018-11-23 16:20:11  Training  186/1000  Loss  1.207\n",
      "2018-11-23 16:20:26  Training  187/1000  Loss  1.204\n",
      "2018-11-23 16:20:44  Training  188/1000  Loss  1.228\n",
      "2018-11-23 16:21:00  Training  189/1000  Loss  1.211\n",
      "2018-11-23 16:21:17  Training  190/1000  Loss  1.185\n",
      "2018-11-23 16:21:33  Training  191/1000  Loss  1.183\n",
      "2018-11-23 16:21:50  Training  192/1000  Loss  1.183\n",
      "2018-11-23 16:22:19  Training  193/1000  Loss  1.162\n",
      "2018-11-23 16:22:45  Training  194/1000  Loss  1.160\n",
      "2018-11-23 16:23:05  Training  195/1000  Loss  1.131\n",
      "2018-11-23 16:23:24  Training  196/1000  Loss  1.103\n",
      "2018-11-23 16:23:43  Training  197/1000  Loss  1.090\n",
      "2018-11-23 16:24:03  Training  198/1000  Loss  1.108\n",
      "2018-11-23 16:24:23  Training  199/1000  Loss  1.110\n",
      "2018-11-23 16:24:42  Training  200/1000  Loss  1.108\n",
      "2018-11-23 16:25:01  Training  201/1000  Loss  1.105\n",
      "2018-11-23 16:25:20  Training  202/1000  Loss  1.110\n",
      "2018-11-23 16:25:36  Training  203/1000  Loss  1.099\n",
      "2018-11-23 16:25:56  Training  204/1000  Loss  1.089\n",
      "2018-11-23 16:26:12  Training  205/1000  Loss  1.068\n",
      "2018-11-23 16:26:31  Training  206/1000  Loss  1.067\n",
      "2018-11-23 16:26:45  Training  207/1000  Loss  1.078\n",
      "2018-11-23 16:27:00  Training  208/1000  Loss  1.055\n",
      "2018-11-23 16:27:15  Training  209/1000  Loss  1.074\n",
      "2018-11-23 16:27:30  Training  210/1000  Loss  1.082\n",
      "2018-11-23 16:27:44  Training  211/1000  Loss  1.104\n",
      "2018-11-23 16:28:00  Training  212/1000  Loss  1.102\n",
      "2018-11-23 16:28:16  Training  213/1000  Loss  1.094\n",
      "2018-11-23 16:28:33  Training  214/1000  Loss  1.083\n",
      "2018-11-23 16:28:51  Training  215/1000  Loss  1.083\n",
      "2018-11-23 16:29:12  Training  216/1000  Loss  1.079\n",
      "2018-11-23 16:29:33  Training  217/1000  Loss  1.084\n",
      "2018-11-23 16:29:49  Training  218/1000  Loss  1.082\n",
      "2018-11-23 16:30:07  Training  219/1000  Loss  1.087\n",
      "2018-11-23 16:30:23  Training  220/1000  Loss  1.074\n",
      "2018-11-23 16:30:39  Training  221/1000  Loss  1.081\n",
      "2018-11-23 16:30:54  Training  222/1000  Loss  1.058\n",
      "2018-11-23 16:31:11  Training  223/1000  Loss  1.068\n",
      "2018-11-23 16:31:28  Training  224/1000  Loss  1.059\n",
      "2018-11-23 16:31:43  Training  225/1000  Loss  1.043\n",
      "2018-11-23 16:32:01  Training  226/1000  Loss  1.048\n",
      "2018-11-23 16:32:17  Training  227/1000  Loss  1.046\n",
      "2018-11-23 16:32:31  Training  228/1000  Loss  1.040\n",
      "2018-11-23 16:32:48  Training  229/1000  Loss  1.018\n",
      "2018-11-23 16:33:03  Training  230/1000  Loss  1.025\n",
      "2018-11-23 16:33:20  Training  231/1000  Loss  1.020\n",
      "2018-11-23 16:33:38  Training  232/1000  Loss  1.029\n",
      "2018-11-23 16:33:54  Training  233/1000  Loss  1.043\n",
      "2018-11-23 16:34:15  Training  234/1000  Loss  1.062\n",
      "2018-11-23 16:34:41  Training  235/1000  Loss  1.075\n",
      "2018-11-23 16:35:01  Training  236/1000  Loss  1.115\n",
      "2018-11-23 16:35:23  Training  237/1000  Loss  1.098\n",
      "2018-11-23 16:35:43  Training  238/1000  Loss  1.093\n",
      "2018-11-23 16:36:02  Training  239/1000  Loss  1.086\n",
      "2018-11-23 16:36:21  Training  240/1000  Loss  1.074\n",
      "2018-11-23 16:36:38  Training  241/1000  Loss  1.059\n",
      "2018-11-23 16:36:58  Training  242/1000  Loss  1.053\n",
      "2018-11-23 16:37:13  Training  243/1000  Loss  1.074\n",
      "2018-11-23 16:37:29  Training  244/1000  Loss  1.056\n",
      "2018-11-23 16:37:50  Training  245/1000  Loss  1.055\n",
      "2018-11-23 16:38:10  Training  246/1000  Loss  1.037\n",
      "2018-11-23 16:38:25  Training  247/1000  Loss  1.015\n",
      "2018-11-23 16:38:40  Training  248/1000  Loss  1.023\n",
      "2018-11-23 16:38:56  Training  249/1000  Loss  1.035\n",
      "2018-11-23 16:39:15  Training  250/1000  Loss  1.018\n",
      "2018-11-23 16:39:38  Training  251/1000  Loss  0.996\n",
      "2018-11-23 16:40:02  Training  252/1000  Loss  0.992\n",
      "2018-11-23 16:40:19  Training  253/1000  Loss  1.024\n",
      "2018-11-23 16:40:38  Training  254/1000  Loss  1.011\n",
      "2018-11-23 16:40:54  Training  255/1000  Loss  1.036\n",
      "2018-11-23 16:41:13  Training  256/1000  Loss  1.000\n",
      "2018-11-23 16:41:30  Training  257/1000  Loss  1.002\n",
      "2018-11-23 16:41:48  Training  258/1000  Loss  1.012\n",
      "2018-11-23 16:42:05  Training  259/1000  Loss  1.012\n",
      "2018-11-23 16:42:19  Training  260/1000  Loss  0.981\n",
      "2018-11-23 16:42:36  Training  261/1000  Loss  0.980\n",
      "2018-11-23 16:42:52  Training  262/1000  Loss  0.984\n",
      "2018-11-23 16:43:09  Training  263/1000  Loss  1.000\n",
      "2018-11-23 16:43:26  Training  264/1000  Loss  0.981\n",
      "2018-11-23 16:43:41  Training  265/1000  Loss  0.967\n",
      "2018-11-23 16:43:56  Training  266/1000  Loss  0.975\n",
      "2018-11-23 16:44:17  Training  267/1000  Loss  0.982\n",
      "2018-11-23 16:44:35  Training  268/1000  Loss  0.977\n",
      "2018-11-23 16:44:53  Training  269/1000  Loss  0.962\n",
      "2018-11-23 16:45:16  Training  270/1000  Loss  0.961\n",
      "2018-11-23 16:45:48  Training  271/1000  Loss  0.947\n",
      "2018-11-23 16:46:20  Training  272/1000  Loss  0.952\n",
      "2018-11-23 16:46:54  Training  273/1000  Loss  0.961\n",
      "2018-11-23 16:47:19  Training  274/1000  Loss  0.961\n",
      "2018-11-23 16:47:39  Training  275/1000  Loss  0.953\n",
      "2018-11-23 16:48:00  Training  276/1000  Loss  0.949\n",
      "2018-11-23 16:48:23  Training  277/1000  Loss  0.965\n",
      "2018-11-23 16:48:43  Training  278/1000  Loss  0.958\n",
      "2018-11-23 16:49:07  Training  279/1000  Loss  0.937\n",
      "2018-11-23 16:49:30  Training  280/1000  Loss  0.950\n",
      "2018-11-23 16:49:53  Training  281/1000  Loss  0.937\n",
      "2018-11-23 16:50:16  Training  282/1000  Loss  0.941\n",
      "2018-11-23 16:50:41  Training  283/1000  Loss  0.948\n",
      "2018-11-23 16:51:01  Training  284/1000  Loss  0.960\n",
      "2018-11-23 16:51:18  Training  285/1000  Loss  0.950\n",
      "2018-11-23 16:51:34  Training  286/1000  Loss  0.952\n",
      "2018-11-23 16:51:50  Training  287/1000  Loss  0.962\n",
      "2018-11-23 16:52:25  Training  288/1000  Loss  0.951\n",
      "2018-11-23 16:52:46  Training  289/1000  Loss  0.968\n",
      "2018-11-23 16:53:05  Training  290/1000  Loss  0.969\n",
      "2018-11-23 16:53:23  Training  291/1000  Loss  0.974\n",
      "2018-11-23 16:53:45  Training  292/1000  Loss  0.962\n",
      "2018-11-23 16:54:11  Training  293/1000  Loss  0.942\n",
      "2018-11-23 16:54:48  Training  294/1000  Loss  0.951\n",
      "2018-11-23 16:55:11  Training  295/1000  Loss  0.932\n",
      "2018-11-23 16:55:28  Training  296/1000  Loss  0.950\n",
      "2018-11-23 16:56:01  Training  297/1000  Loss  0.951\n",
      "2018-11-23 16:56:22  Training  298/1000  Loss  0.961\n",
      "2018-11-23 16:56:41  Training  299/1000  Loss  0.940\n",
      "2018-11-23 16:57:01  Training  300/1000  Loss  0.943\n",
      "2018-11-23 16:57:15  Training  301/1000  Loss  0.953\n",
      "2018-11-23 16:57:31  Training  302/1000  Loss  0.977\n",
      "2018-11-23 16:58:03  Training  303/1000  Loss  0.949\n",
      "2018-11-23 16:58:23  Training  304/1000  Loss  0.935\n",
      "2018-11-23 16:58:38  Training  305/1000  Loss  0.946\n",
      "2018-11-23 16:58:53  Training  306/1000  Loss  0.926\n",
      "2018-11-23 16:59:14  Training  307/1000  Loss  0.932\n",
      "2018-11-23 16:59:31  Training  308/1000  Loss  0.907\n",
      "2018-11-23 16:59:46  Training  309/1000  Loss  0.894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-23 17:00:01  Training  310/1000  Loss  0.901\n",
      "2018-11-23 17:00:18  Training  311/1000  Loss  0.891\n",
      "2018-11-23 17:00:33  Training  312/1000  Loss  0.905\n",
      "2018-11-23 17:00:47  Training  313/1000  Loss  0.912\n",
      "2018-11-23 17:01:01  Training  314/1000  Loss  0.903\n",
      "2018-11-23 17:01:16  Training  315/1000  Loss  0.902\n",
      "2018-11-23 17:01:31  Training  316/1000  Loss  0.901\n",
      "2018-11-23 17:01:45  Training  317/1000  Loss  0.894\n",
      "2018-11-23 17:02:01  Training  318/1000  Loss  0.890\n",
      "2018-11-23 17:02:16  Training  319/1000  Loss  0.883\n",
      "2018-11-23 17:02:33  Training  320/1000  Loss  0.887\n",
      "2018-11-23 17:02:48  Training  321/1000  Loss  0.894\n",
      "2018-11-23 17:03:09  Training  322/1000  Loss  0.887\n",
      "2018-11-23 17:03:23  Training  323/1000  Loss  0.874\n",
      "2018-11-23 17:03:39  Training  324/1000  Loss  0.888\n",
      "2018-11-23 17:03:53  Training  325/1000  Loss  0.874\n",
      "2018-11-23 17:04:08  Training  326/1000  Loss  0.872\n",
      "2018-11-23 17:04:25  Training  327/1000  Loss  0.899\n",
      "2018-11-23 17:04:39  Training  328/1000  Loss  0.927\n",
      "2018-11-23 17:04:55  Training  329/1000  Loss  0.910\n",
      "2018-11-23 17:05:23  Training  330/1000  Loss  0.930\n",
      "2018-11-23 17:05:42  Training  331/1000  Loss  0.911\n",
      "2018-11-23 17:06:00  Training  332/1000  Loss  0.910\n",
      "2018-11-23 17:06:15  Training  333/1000  Loss  0.909\n",
      "2018-11-23 17:06:32  Training  334/1000  Loss  0.901\n",
      "2018-11-23 17:06:48  Training  335/1000  Loss  0.901\n",
      "2018-11-23 17:07:04  Training  336/1000  Loss  0.902\n",
      "2018-11-23 17:07:20  Training  337/1000  Loss  0.896\n",
      "2018-11-23 17:07:36  Training  338/1000  Loss  0.887\n",
      "2018-11-23 17:07:50  Training  339/1000  Loss  0.899\n",
      "2018-11-23 17:08:06  Training  340/1000  Loss  0.898\n",
      "2018-11-23 17:08:21  Training  341/1000  Loss  0.924\n",
      "2018-11-23 17:08:35  Training  342/1000  Loss  0.909\n",
      "2018-11-23 17:08:52  Training  343/1000  Loss  0.908\n",
      "2018-11-23 17:09:06  Training  344/1000  Loss  0.905\n",
      "2018-11-23 17:09:21  Training  345/1000  Loss  0.897\n",
      "2018-11-23 17:09:38  Training  346/1000  Loss  0.909\n",
      "2018-11-23 17:09:57  Training  347/1000  Loss  0.920\n",
      "2018-11-23 17:10:12  Training  348/1000  Loss  0.926\n",
      "2018-11-23 17:10:27  Training  349/1000  Loss  0.917\n",
      "2018-11-23 17:10:43  Training  350/1000  Loss  0.912\n",
      "2018-11-23 17:10:58  Training  351/1000  Loss  0.942\n",
      "2018-11-23 17:11:13  Training  352/1000  Loss  0.921\n",
      "2018-11-23 17:11:28  Training  353/1000  Loss  0.914\n",
      "2018-11-23 17:11:43  Training  354/1000  Loss  0.915\n",
      "2018-11-23 17:12:01  Training  355/1000  Loss  0.904\n",
      "2018-11-23 17:12:15  Training  356/1000  Loss  0.889\n",
      "2018-11-23 17:12:30  Training  357/1000  Loss  0.872\n",
      "2018-11-23 17:12:44  Training  358/1000  Loss  0.869\n",
      "2018-11-23 17:13:02  Training  359/1000  Loss  0.893\n",
      "2018-11-23 17:13:16  Training  360/1000  Loss  0.883\n",
      "2018-11-23 17:13:31  Training  361/1000  Loss  0.882\n",
      "2018-11-23 17:13:47  Training  362/1000  Loss  0.872\n",
      "2018-11-23 17:14:07  Training  363/1000  Loss  0.884\n",
      "2018-11-23 17:14:26  Training  364/1000  Loss  0.874\n",
      "2018-11-23 17:14:44  Training  365/1000  Loss  0.870\n",
      "2018-11-23 17:14:58  Training  366/1000  Loss  0.871\n",
      "2018-11-23 17:15:20  Training  367/1000  Loss  0.879\n",
      "2018-11-23 17:15:37  Training  368/1000  Loss  0.869\n",
      "2018-11-23 17:16:01  Training  369/1000  Loss  0.885\n",
      "2018-11-23 17:16:18  Training  370/1000  Loss  0.871\n",
      "2018-11-23 17:16:33  Training  371/1000  Loss  0.866\n",
      "2018-11-23 17:16:48  Training  372/1000  Loss  0.878\n",
      "2018-11-23 17:17:03  Training  373/1000  Loss  0.893\n",
      "2018-11-23 17:17:17  Training  374/1000  Loss  0.893\n",
      "2018-11-23 17:17:33  Training  375/1000  Loss  0.883\n",
      "2018-11-23 17:17:53  Training  376/1000  Loss  0.856\n",
      "2018-11-23 17:18:11  Training  377/1000  Loss  0.850\n",
      "2018-11-23 17:18:29  Training  378/1000  Loss  0.827\n",
      "2018-11-23 17:18:49  Training  379/1000  Loss  0.824\n",
      "2018-11-23 17:19:09  Training  380/1000  Loss  0.823\n",
      "2018-11-23 17:19:29  Training  381/1000  Loss  0.821\n",
      "2018-11-23 17:19:44  Training  382/1000  Loss  0.814\n",
      "2018-11-23 17:20:00  Training  383/1000  Loss  0.818\n",
      "2018-11-23 17:20:16  Training  384/1000  Loss  0.821\n",
      "2018-11-23 17:20:31  Training  385/1000  Loss  0.815\n",
      "2018-11-23 17:20:51  Training  386/1000  Loss  0.823\n",
      "2018-11-23 17:21:05  Training  387/1000  Loss  0.818\n",
      "2018-11-23 17:21:31  Training  388/1000  Loss  0.803\n",
      "2018-11-23 17:21:45  Training  389/1000  Loss  0.788\n",
      "2018-11-23 17:22:01  Training  390/1000  Loss  0.812\n",
      "2018-11-23 17:22:16  Training  391/1000  Loss  0.815\n",
      "2018-11-23 17:22:33  Training  392/1000  Loss  0.816\n",
      "2018-11-23 17:22:48  Training  393/1000  Loss  0.812\n",
      "2018-11-23 17:23:04  Training  394/1000  Loss  0.822\n",
      "2018-11-23 17:23:20  Training  395/1000  Loss  0.849\n",
      "2018-11-23 17:23:42  Training  396/1000  Loss  0.841\n",
      "2018-11-23 17:23:58  Training  397/1000  Loss  0.821\n",
      "2018-11-23 17:24:13  Training  398/1000  Loss  0.825\n",
      "2018-11-23 17:24:33  Training  399/1000  Loss  0.818\n",
      "2018-11-23 17:24:51  Training  400/1000  Loss  0.801\n",
      "2018-11-23 17:25:05  Training  401/1000  Loss  0.831\n",
      "2018-11-23 17:25:22  Training  402/1000  Loss  0.848\n",
      "2018-11-23 17:25:36  Training  403/1000  Loss  0.838\n",
      "2018-11-23 17:25:53  Training  404/1000  Loss  0.824\n",
      "2018-11-23 17:26:10  Training  405/1000  Loss  0.825\n",
      "2018-11-23 17:26:24  Training  406/1000  Loss  0.822\n",
      "2018-11-23 17:26:39  Training  407/1000  Loss  0.824\n",
      "2018-11-23 17:26:55  Training  408/1000  Loss  0.815\n",
      "2018-11-23 17:27:11  Training  409/1000  Loss  0.814\n",
      "2018-11-23 17:27:26  Training  410/1000  Loss  0.801\n",
      "2018-11-23 17:27:45  Training  411/1000  Loss  0.781\n",
      "2018-11-23 17:28:01  Training  412/1000  Loss  0.789\n",
      "2018-11-23 17:28:24  Training  413/1000  Loss  0.819\n",
      "2018-11-23 17:28:39  Training  414/1000  Loss  0.817\n",
      "2018-11-23 17:28:59  Training  415/1000  Loss  0.811\n",
      "2018-11-23 17:29:19  Training  416/1000  Loss  0.799\n",
      "2018-11-23 17:29:39  Training  417/1000  Loss  0.815\n",
      "2018-11-23 17:29:59  Training  418/1000  Loss  0.811\n",
      "2018-11-23 17:30:16  Training  419/1000  Loss  0.811\n",
      "2018-11-23 17:30:32  Training  420/1000  Loss  0.811\n",
      "2018-11-23 17:30:46  Training  421/1000  Loss  0.815\n",
      "2018-11-23 17:31:03  Training  422/1000  Loss  0.800\n",
      "2018-11-23 17:31:18  Training  423/1000  Loss  0.782\n",
      "2018-11-23 17:31:34  Training  424/1000  Loss  0.791\n",
      "2018-11-23 17:31:50  Training  425/1000  Loss  0.801\n",
      "2018-11-23 17:32:05  Training  426/1000  Loss  0.819\n",
      "2018-11-23 17:32:24  Training  427/1000  Loss  0.823\n",
      "2018-11-23 17:32:45  Training  428/1000  Loss  0.810\n",
      "2018-11-23 17:32:59  Training  429/1000  Loss  0.829\n",
      "2018-11-23 17:33:15  Training  430/1000  Loss  0.814\n",
      "2018-11-23 17:33:29  Training  431/1000  Loss  0.827\n",
      "2018-11-23 17:33:48  Training  432/1000  Loss  0.814\n",
      "2018-11-23 17:34:05  Training  433/1000  Loss  0.805\n",
      "2018-11-23 17:34:22  Training  434/1000  Loss  0.812\n",
      "2018-11-23 17:34:41  Training  435/1000  Loss  0.801\n",
      "2018-11-23 17:34:57  Training  436/1000  Loss  0.821\n",
      "2018-11-23 17:35:16  Training  437/1000  Loss  0.815\n",
      "2018-11-23 17:35:36  Training  438/1000  Loss  0.799\n",
      "2018-11-23 17:35:55  Training  439/1000  Loss  0.778\n",
      "2018-11-23 17:36:13  Training  440/1000  Loss  0.788\n",
      "2018-11-23 17:36:30  Training  441/1000  Loss  0.782\n",
      "2018-11-23 17:36:44  Training  442/1000  Loss  0.776\n",
      "2018-11-23 17:37:00  Training  443/1000  Loss  0.767\n",
      "2018-11-23 17:37:19  Training  444/1000  Loss  0.762\n",
      "2018-11-23 17:37:34  Training  445/1000  Loss  0.772\n",
      "2018-11-23 17:37:49  Training  446/1000  Loss  0.765\n",
      "2018-11-23 17:38:07  Training  447/1000  Loss  0.756\n",
      "2018-11-23 17:38:21  Training  448/1000  Loss  0.744\n",
      "2018-11-23 17:38:36  Training  449/1000  Loss  0.737\n",
      "2018-11-23 17:38:52  Training  450/1000  Loss  0.777\n",
      "2018-11-23 17:39:06  Training  451/1000  Loss  0.773\n",
      "2018-11-23 17:39:22  Training  452/1000  Loss  0.772\n",
      "2018-11-23 17:39:38  Training  453/1000  Loss  0.775\n",
      "2018-11-23 17:39:54  Training  454/1000  Loss  0.782\n",
      "2018-11-23 17:40:09  Training  455/1000  Loss  0.786\n",
      "2018-11-23 17:40:26  Training  456/1000  Loss  0.788\n",
      "2018-11-23 17:40:41  Training  457/1000  Loss  0.792\n",
      "2018-11-23 17:41:01  Training  458/1000  Loss  0.774\n",
      "2018-11-23 17:41:15  Training  459/1000  Loss  0.795\n",
      "2018-11-23 17:41:32  Training  460/1000  Loss  0.808\n",
      "2018-11-23 17:41:47  Training  461/1000  Loss  0.784\n",
      "2018-11-23 17:42:05  Training  462/1000  Loss  0.785\n",
      "2018-11-23 17:42:20  Training  463/1000  Loss  0.769\n",
      "2018-11-23 17:42:37  Training  464/1000  Loss  0.792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-23 17:42:52  Training  465/1000  Loss  0.772\n",
      "2018-11-23 17:43:08  Training  466/1000  Loss  0.764\n",
      "2018-11-23 17:43:22  Training  467/1000  Loss  0.768\n",
      "2018-11-23 17:43:42  Training  468/1000  Loss  0.752\n",
      "2018-11-23 17:43:57  Training  469/1000  Loss  0.747\n",
      "2018-11-23 17:44:13  Training  470/1000  Loss  0.775\n",
      "2018-11-23 17:44:33  Training  471/1000  Loss  0.785\n",
      "2018-11-23 17:44:48  Training  472/1000  Loss  0.791\n",
      "2018-11-23 17:45:05  Training  473/1000  Loss  0.793\n",
      "2018-11-23 17:45:19  Training  474/1000  Loss  0.781\n",
      "2018-11-23 17:45:41  Training  475/1000  Loss  0.771\n",
      "2018-11-23 17:46:02  Training  476/1000  Loss  0.781\n",
      "2018-11-23 17:46:31  Training  477/1000  Loss  0.768\n",
      "2018-11-23 17:46:51  Training  478/1000  Loss  0.773\n",
      "2018-11-23 17:47:18  Training  479/1000  Loss  0.775\n",
      "2018-11-23 17:47:40  Training  480/1000  Loss  0.765\n",
      "2018-11-23 17:47:54  Training  481/1000  Loss  0.752\n",
      "2018-11-23 17:48:11  Training  482/1000  Loss  0.765\n",
      "2018-11-23 17:48:26  Training  483/1000  Loss  0.792\n",
      "2018-11-23 17:48:44  Training  484/1000  Loss  0.795\n",
      "2018-11-23 17:49:01  Training  485/1000  Loss  0.807\n",
      "2018-11-23 17:49:16  Training  486/1000  Loss  0.799\n",
      "2018-11-23 17:49:35  Training  487/1000  Loss  0.789\n",
      "2018-11-23 17:49:51  Training  488/1000  Loss  0.773\n",
      "2018-11-23 17:50:10  Training  489/1000  Loss  0.777\n",
      "2018-11-23 17:50:25  Training  490/1000  Loss  0.782\n",
      "2018-11-23 17:50:40  Training  491/1000  Loss  0.763\n",
      "2018-11-23 17:50:55  Training  492/1000  Loss  0.766\n",
      "2018-11-23 17:51:09  Training  493/1000  Loss  0.763\n",
      "2018-11-23 17:51:38  Training  494/1000  Loss  0.768\n",
      "2018-11-23 17:51:59  Training  495/1000  Loss  0.759\n",
      "2018-11-23 17:52:18  Training  496/1000  Loss  0.765\n",
      "2018-11-23 17:52:32  Training  497/1000  Loss  0.764\n",
      "2018-11-23 17:52:46  Training  498/1000  Loss  0.745\n",
      "2018-11-23 17:53:02  Training  499/1000  Loss  0.752\n",
      "2018-11-23 17:53:18  Training  500/1000  Loss  0.737\n",
      "2018-11-23 17:53:33  Training  501/1000  Loss  0.737\n",
      "2018-11-23 17:53:49  Training  502/1000  Loss  0.728\n",
      "2018-11-23 17:54:05  Training  503/1000  Loss  0.723\n",
      "2018-11-23 17:54:21  Training  504/1000  Loss  0.727\n",
      "2018-11-23 17:54:39  Training  505/1000  Loss  0.723\n",
      "2018-11-23 17:54:58  Training  506/1000  Loss  0.737\n",
      "2018-11-23 17:55:14  Training  507/1000  Loss  0.747\n",
      "2018-11-23 17:55:31  Training  508/1000  Loss  0.727\n",
      "2018-11-23 17:55:46  Training  509/1000  Loss  0.748\n",
      "2018-11-23 17:56:02  Training  510/1000  Loss  0.767\n",
      "2018-11-23 17:56:16  Training  511/1000  Loss  0.788\n",
      "2018-11-23 17:56:34  Training  512/1000  Loss  0.772\n",
      "2018-11-23 17:56:49  Training  513/1000  Loss  0.779\n",
      "2018-11-23 17:57:05  Training  514/1000  Loss  0.782\n",
      "2018-11-23 17:57:26  Training  515/1000  Loss  0.787\n",
      "2018-11-23 17:57:43  Training  516/1000  Loss  0.789\n",
      "2018-11-23 17:58:02  Training  517/1000  Loss  0.778\n",
      "2018-11-23 17:58:20  Training  518/1000  Loss  0.760\n",
      "2018-11-23 17:58:41  Training  519/1000  Loss  0.749\n",
      "2018-11-23 17:58:57  Training  520/1000  Loss  0.754\n",
      "2018-11-23 17:59:12  Training  521/1000  Loss  0.751\n",
      "2018-11-23 17:59:33  Training  522/1000  Loss  0.748\n",
      "2018-11-23 17:59:51  Training  523/1000  Loss  0.744\n",
      "2018-11-23 18:00:08  Training  524/1000  Loss  0.741\n",
      "2018-11-23 18:00:22  Training  525/1000  Loss  0.731\n",
      "2018-11-23 18:00:39  Training  526/1000  Loss  0.717\n",
      "2018-11-23 18:00:53  Training  527/1000  Loss  0.709\n",
      "2018-11-23 18:01:08  Training  528/1000  Loss  0.712\n",
      "2018-11-23 18:01:22  Training  529/1000  Loss  0.722\n",
      "2018-11-23 18:01:37  Training  530/1000  Loss  0.724\n",
      "2018-11-23 18:01:52  Training  531/1000  Loss  0.735\n",
      "2018-11-23 18:02:10  Training  532/1000  Loss  0.742\n",
      "2018-11-23 18:02:23  Training  533/1000  Loss  0.725\n",
      "2018-11-23 18:02:38  Training  534/1000  Loss  0.735\n",
      "2018-11-23 18:02:54  Training  535/1000  Loss  0.739\n",
      "2018-11-23 18:03:08  Training  536/1000  Loss  0.728\n",
      "2018-11-23 18:03:23  Training  537/1000  Loss  0.730\n",
      "2018-11-23 18:03:37  Training  538/1000  Loss  0.723\n",
      "2018-11-23 18:03:52  Training  539/1000  Loss  0.704\n",
      "2018-11-23 18:04:08  Training  540/1000  Loss  0.713\n",
      "2018-11-23 18:04:23  Training  541/1000  Loss  0.704\n",
      "2018-11-23 18:04:38  Training  542/1000  Loss  0.692\n",
      "2018-11-23 18:04:58  Training  543/1000  Loss  0.729\n",
      "2018-11-23 18:05:13  Training  544/1000  Loss  0.720\n",
      "2018-11-23 18:05:43  Training  545/1000  Loss  0.711\n",
      "2018-11-23 18:06:02  Training  546/1000  Loss  0.721\n",
      "2018-11-23 18:06:17  Training  547/1000  Loss  0.702\n",
      "2018-11-23 18:06:32  Training  548/1000  Loss  0.694\n",
      "2018-11-23 18:06:46  Training  549/1000  Loss  0.703\n",
      "2018-11-23 18:07:00  Training  550/1000  Loss  0.711\n",
      "2018-11-23 18:07:15  Training  551/1000  Loss  0.700\n",
      "2018-11-23 18:07:30  Training  552/1000  Loss  0.687\n",
      "2018-11-23 18:07:46  Training  553/1000  Loss  0.682\n",
      "2018-11-23 18:08:03  Training  554/1000  Loss  0.685\n",
      "2018-11-23 18:08:19  Training  555/1000  Loss  0.671\n",
      "2018-11-23 18:08:34  Training  556/1000  Loss  0.680\n",
      "2018-11-23 18:08:50  Training  557/1000  Loss  0.687\n",
      "2018-11-23 18:09:10  Training  558/1000  Loss  0.694\n",
      "2018-11-23 18:09:29  Training  559/1000  Loss  0.706\n",
      "2018-11-23 18:09:45  Training  560/1000  Loss  0.695\n",
      "2018-11-23 18:10:03  Training  561/1000  Loss  0.684\n",
      "2018-11-23 18:10:18  Training  562/1000  Loss  0.685\n",
      "2018-11-23 18:10:34  Training  563/1000  Loss  0.704\n",
      "2018-11-23 18:10:49  Training  564/1000  Loss  0.702\n",
      "2018-11-23 18:11:09  Training  565/1000  Loss  0.676\n",
      "2018-11-23 18:11:24  Training  566/1000  Loss  0.693\n",
      "2018-11-23 18:11:39  Training  567/1000  Loss  0.686\n",
      "2018-11-23 18:11:59  Training  568/1000  Loss  0.665\n",
      "2018-11-23 18:12:19  Training  569/1000  Loss  0.650\n",
      "2018-11-23 18:12:33  Training  570/1000  Loss  0.658\n",
      "2018-11-23 18:12:48  Training  571/1000  Loss  0.652\n",
      "2018-11-23 18:13:02  Training  572/1000  Loss  0.653\n",
      "2018-11-23 18:13:18  Training  573/1000  Loss  0.692\n",
      "2018-11-23 18:13:38  Training  574/1000  Loss  0.698\n",
      "2018-11-23 18:13:53  Training  575/1000  Loss  0.691\n",
      "2018-11-23 18:14:11  Training  576/1000  Loss  0.689\n",
      "2018-11-23 18:14:26  Training  577/1000  Loss  0.687\n",
      "2018-11-23 18:14:46  Training  578/1000  Loss  0.679\n",
      "2018-11-23 18:15:00  Training  579/1000  Loss  0.687\n",
      "2018-11-23 18:15:23  Training  580/1000  Loss  0.695\n",
      "2018-11-23 18:15:39  Training  581/1000  Loss  0.698\n",
      "2018-11-23 18:15:59  Training  582/1000  Loss  0.675\n",
      "2018-11-23 18:16:16  Training  583/1000  Loss  0.657\n",
      "2018-11-23 18:16:32  Training  584/1000  Loss  0.655\n",
      "2018-11-23 18:16:47  Training  585/1000  Loss  0.670\n",
      "2018-11-23 18:17:02  Training  586/1000  Loss  0.664\n",
      "2018-11-23 18:17:18  Training  587/1000  Loss  0.665\n",
      "2018-11-23 18:17:32  Training  588/1000  Loss  0.658\n",
      "2018-11-23 18:17:48  Training  589/1000  Loss  0.661\n",
      "2018-11-23 18:18:06  Training  590/1000  Loss  0.670\n",
      "2018-11-23 18:18:24  Training  591/1000  Loss  0.681\n",
      "2018-11-23 18:18:38  Training  592/1000  Loss  0.685\n",
      "2018-11-23 18:18:52  Training  593/1000  Loss  0.670\n",
      "2018-11-23 18:19:08  Training  594/1000  Loss  0.667\n",
      "2018-11-23 18:19:24  Training  595/1000  Loss  0.653\n",
      "2018-11-23 18:19:41  Training  596/1000  Loss  0.651\n",
      "2018-11-23 18:19:55  Training  597/1000  Loss  0.669\n",
      "2018-11-23 18:20:11  Training  598/1000  Loss  0.659\n",
      "2018-11-23 18:20:25  Training  599/1000  Loss  0.670\n",
      "2018-11-23 18:20:39  Training  600/1000  Loss  0.668\n",
      "2018-11-23 18:20:55  Training  601/1000  Loss  0.652\n",
      "2018-11-23 18:21:11  Training  602/1000  Loss  0.643\n",
      "2018-11-23 18:21:28  Training  603/1000  Loss  0.633\n",
      "2018-11-23 18:21:46  Training  604/1000  Loss  0.624\n",
      "2018-11-23 18:22:01  Training  605/1000  Loss  0.628\n",
      "2018-11-23 18:22:16  Training  606/1000  Loss  0.638\n",
      "2018-11-23 18:22:32  Training  607/1000  Loss  0.645\n",
      "2018-11-23 18:22:47  Training  608/1000  Loss  0.644\n",
      "2018-11-23 18:23:04  Training  609/1000  Loss  0.658\n",
      "2018-11-23 18:23:20  Training  610/1000  Loss  0.670\n",
      "2018-11-23 18:23:34  Training  611/1000  Loss  0.648\n",
      "2018-11-23 18:23:48  Training  612/1000  Loss  0.645\n",
      "2018-11-23 18:24:03  Training  613/1000  Loss  0.649\n",
      "2018-11-23 18:24:19  Training  614/1000  Loss  0.673\n",
      "2018-11-23 18:24:36  Training  615/1000  Loss  0.654\n",
      "2018-11-23 18:24:53  Training  616/1000  Loss  0.662\n",
      "2018-11-23 18:25:08  Training  617/1000  Loss  0.645\n",
      "2018-11-23 18:25:23  Training  618/1000  Loss  0.646\n",
      "2018-11-23 18:25:37  Training  619/1000  Loss  0.666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-23 18:25:51  Training  620/1000  Loss  0.684\n",
      "2018-11-23 18:26:09  Training  621/1000  Loss  0.674\n",
      "2018-11-23 18:26:22  Training  622/1000  Loss  0.671\n",
      "2018-11-23 18:26:37  Training  623/1000  Loss  0.661\n",
      "2018-11-23 18:26:52  Training  624/1000  Loss  0.657\n",
      "2018-11-23 18:27:08  Training  625/1000  Loss  0.671\n",
      "2018-11-23 18:27:24  Training  626/1000  Loss  0.663\n",
      "2018-11-23 18:27:40  Training  627/1000  Loss  0.657\n",
      "2018-11-23 18:27:55  Training  628/1000  Loss  0.655\n",
      "2018-11-23 18:28:13  Training  629/1000  Loss  0.651\n",
      "2018-11-23 18:28:39  Training  630/1000  Loss  0.637\n",
      "2018-11-23 18:28:53  Training  631/1000  Loss  0.641\n",
      "2018-11-23 18:29:11  Training  632/1000  Loss  0.636\n",
      "2018-11-23 18:29:29  Training  633/1000  Loss  0.642\n",
      "2018-11-23 18:29:47  Training  634/1000  Loss  0.628\n",
      "2018-11-23 18:30:06  Training  635/1000  Loss  0.630\n",
      "2018-11-23 18:30:20  Training  636/1000  Loss  0.622\n",
      "2018-11-23 18:30:35  Training  637/1000  Loss  0.625\n",
      "2018-11-23 18:30:51  Training  638/1000  Loss  0.614\n",
      "2018-11-23 18:31:09  Training  639/1000  Loss  0.604\n",
      "2018-11-23 18:31:23  Training  640/1000  Loss  0.605\n",
      "2018-11-23 18:31:38  Training  641/1000  Loss  0.601\n",
      "2018-11-23 18:31:53  Training  642/1000  Loss  0.616\n",
      "2018-11-23 18:32:11  Training  643/1000  Loss  0.631\n",
      "2018-11-23 18:32:27  Training  644/1000  Loss  0.616\n",
      "2018-11-23 18:32:41  Training  645/1000  Loss  0.620\n",
      "2018-11-23 18:32:56  Training  646/1000  Loss  0.640\n",
      "2018-11-23 18:33:10  Training  647/1000  Loss  0.630\n",
      "2018-11-23 18:33:25  Training  648/1000  Loss  0.626\n",
      "2018-11-23 18:33:39  Training  649/1000  Loss  0.609\n",
      "2018-11-23 18:33:55  Training  650/1000  Loss  0.611\n",
      "2018-11-23 18:34:13  Training  651/1000  Loss  0.605\n",
      "2018-11-23 18:34:29  Training  652/1000  Loss  0.611\n",
      "2018-11-23 18:34:47  Training  653/1000  Loss  0.619\n",
      "2018-11-23 18:35:01  Training  654/1000  Loss  0.614\n",
      "2018-11-23 18:35:16  Training  655/1000  Loss  0.618\n",
      "2018-11-23 18:35:34  Training  656/1000  Loss  0.616\n",
      "2018-11-23 18:36:00  Training  657/1000  Loss  0.625\n",
      "2018-11-23 18:36:17  Training  658/1000  Loss  0.633\n",
      "2018-11-23 18:36:32  Training  659/1000  Loss  0.632\n",
      "2018-11-23 18:36:50  Training  660/1000  Loss  0.615\n",
      "2018-11-23 18:37:04  Training  661/1000  Loss  0.622\n",
      "2018-11-23 18:37:30  Training  662/1000  Loss  0.615\n",
      "2018-11-23 18:37:48  Training  663/1000  Loss  0.609\n",
      "2018-11-23 18:38:08  Training  664/1000  Loss  0.610\n",
      "2018-11-23 18:38:25  Training  665/1000  Loss  0.600\n",
      "2018-11-23 18:38:41  Training  666/1000  Loss  0.611\n",
      "2018-11-23 18:39:06  Training  667/1000  Loss  0.614\n",
      "2018-11-23 18:39:31  Training  668/1000  Loss  0.605\n",
      "2018-11-23 18:39:50  Training  669/1000  Loss  0.598\n",
      "2018-11-23 18:40:10  Training  670/1000  Loss  0.599\n",
      "2018-11-23 18:40:26  Training  671/1000  Loss  0.622\n",
      "2018-11-23 18:40:40  Training  672/1000  Loss  0.613\n",
      "2018-11-23 18:40:56  Training  673/1000  Loss  0.609\n",
      "2018-11-23 18:41:10  Training  674/1000  Loss  0.632\n",
      "2018-11-23 18:41:26  Training  675/1000  Loss  0.633\n",
      "2018-11-23 18:41:56  Training  676/1000  Loss  0.637\n",
      "2018-11-23 18:42:15  Training  677/1000  Loss  0.645\n",
      "2018-11-23 18:42:32  Training  678/1000  Loss  0.634\n",
      "2018-11-23 18:42:46  Training  679/1000  Loss  0.629\n",
      "2018-11-23 18:43:01  Training  680/1000  Loss  0.650\n",
      "2018-11-23 18:43:15  Training  681/1000  Loss  0.644\n",
      "2018-11-23 18:43:32  Training  682/1000  Loss  0.635\n",
      "2018-11-23 18:43:49  Training  683/1000  Loss  0.630\n",
      "2018-11-23 18:44:08  Training  684/1000  Loss  0.615\n",
      "2018-11-23 18:44:30  Training  685/1000  Loss  0.614\n",
      "2018-11-23 18:44:45  Training  686/1000  Loss  0.645\n",
      "2018-11-23 18:45:01  Training  687/1000  Loss  0.626\n",
      "2018-11-23 18:45:19  Training  688/1000  Loss  0.609\n",
      "2018-11-23 18:45:34  Training  689/1000  Loss  0.601\n",
      "2018-11-23 18:45:48  Training  690/1000  Loss  0.596\n",
      "2018-11-23 18:46:04  Training  691/1000  Loss  0.590\n",
      "2018-11-23 18:46:24  Training  692/1000  Loss  0.595\n",
      "2018-11-23 18:46:54  Training  693/1000  Loss  0.586\n",
      "2018-11-23 18:47:18  Training  694/1000  Loss  0.573\n",
      "2018-11-23 18:47:36  Training  695/1000  Loss  0.572\n",
      "2018-11-23 18:47:53  Training  696/1000  Loss  0.585\n",
      "2018-11-23 18:48:12  Training  697/1000  Loss  0.602\n",
      "2018-11-23 18:48:26  Training  698/1000  Loss  0.603\n",
      "2018-11-23 18:48:42  Training  699/1000  Loss  0.615\n",
      "2018-11-23 18:48:57  Training  700/1000  Loss  0.616\n",
      "2018-11-23 18:49:13  Training  701/1000  Loss  0.622\n",
      "2018-11-23 18:49:33  Training  702/1000  Loss  0.620\n",
      "2018-11-23 18:49:53  Training  703/1000  Loss  0.619\n",
      "2018-11-23 18:50:08  Training  704/1000  Loss  0.620\n",
      "2018-11-23 18:50:25  Training  705/1000  Loss  0.604\n",
      "2018-11-23 18:50:39  Training  706/1000  Loss  0.601\n",
      "2018-11-23 18:50:53  Training  707/1000  Loss  0.607\n",
      "2018-11-23 18:51:10  Training  708/1000  Loss  0.630\n",
      "2018-11-23 18:51:24  Training  709/1000  Loss  0.622\n",
      "2018-11-23 18:51:38  Training  710/1000  Loss  0.612\n",
      "2018-11-23 18:51:59  Training  711/1000  Loss  0.605\n",
      "2018-11-23 18:52:14  Training  712/1000  Loss  0.619\n",
      "2018-11-23 18:52:28  Training  713/1000  Loss  0.641\n",
      "2018-11-23 18:52:43  Training  714/1000  Loss  0.644\n",
      "2018-11-23 18:52:59  Training  715/1000  Loss  0.646\n",
      "2018-11-23 18:53:18  Training  716/1000  Loss  0.627\n",
      "2018-11-23 18:53:34  Training  717/1000  Loss  0.626\n",
      "2018-11-23 18:53:49  Training  718/1000  Loss  0.642\n",
      "2018-11-23 18:54:05  Training  719/1000  Loss  0.645\n",
      "2018-11-23 18:54:20  Training  720/1000  Loss  0.640\n",
      "2018-11-23 18:54:35  Training  721/1000  Loss  0.646\n",
      "2018-11-23 18:54:50  Training  722/1000  Loss  0.647\n",
      "2018-11-23 18:55:03  Training  723/1000  Loss  0.644\n",
      "2018-11-23 18:55:20  Training  724/1000  Loss  0.663\n",
      "2018-11-23 18:55:33  Training  725/1000  Loss  0.645\n",
      "2018-11-23 18:55:50  Training  726/1000  Loss  0.630\n",
      "2018-11-23 18:56:10  Training  727/1000  Loss  0.624\n",
      "2018-11-23 18:56:25  Training  728/1000  Loss  0.619\n",
      "2018-11-23 18:56:39  Training  729/1000  Loss  0.625\n",
      "2018-11-23 18:56:54  Training  730/1000  Loss  0.620\n",
      "2018-11-23 18:57:08  Training  731/1000  Loss  0.620\n",
      "2018-11-23 18:57:27  Training  732/1000  Loss  0.632\n",
      "2018-11-23 18:57:43  Training  733/1000  Loss  0.658\n",
      "2018-11-23 18:57:59  Training  734/1000  Loss  0.650\n",
      "2018-11-23 18:58:14  Training  735/1000  Loss  0.641\n",
      "2018-11-23 18:58:29  Training  736/1000  Loss  0.645\n",
      "2018-11-23 18:58:44  Training  737/1000  Loss  0.636\n",
      "2018-11-23 18:58:58  Training  738/1000  Loss  0.639\n",
      "2018-11-23 18:59:14  Training  739/1000  Loss  0.641\n",
      "2018-11-23 18:59:38  Training  740/1000  Loss  0.624\n",
      "2018-11-23 18:59:53  Training  741/1000  Loss  0.630\n",
      "2018-11-23 19:00:10  Training  742/1000  Loss  0.623\n",
      "2018-11-23 19:00:25  Training  743/1000  Loss  0.659\n",
      "2018-11-23 19:00:42  Training  744/1000  Loss  0.646\n",
      "2018-11-23 19:00:56  Training  745/1000  Loss  0.662\n",
      "2018-11-23 19:01:14  Training  746/1000  Loss  0.659\n",
      "2018-11-23 19:01:30  Training  747/1000  Loss  0.658\n",
      "2018-11-23 19:01:44  Training  748/1000  Loss  0.640\n",
      "2018-11-23 19:02:08  Training  749/1000  Loss  0.630\n",
      "2018-11-23 19:02:25  Training  750/1000  Loss  0.634\n",
      "2018-11-23 19:02:43  Training  751/1000  Loss  0.654\n",
      "2018-11-23 19:02:56  Training  752/1000  Loss  0.644\n",
      "2018-11-23 19:03:12  Training  753/1000  Loss  0.636\n",
      "2018-11-23 19:03:29  Training  754/1000  Loss  0.624\n",
      "2018-11-23 19:03:43  Training  755/1000  Loss  0.610\n",
      "2018-11-23 19:04:02  Training  756/1000  Loss  0.605\n",
      "2018-11-23 19:04:16  Training  757/1000  Loss  0.606\n",
      "2018-11-23 19:04:36  Training  758/1000  Loss  0.590\n",
      "2018-11-23 19:04:52  Training  759/1000  Loss  0.598\n",
      "2018-11-23 19:05:07  Training  760/1000  Loss  0.591\n",
      "2018-11-23 19:05:21  Training  761/1000  Loss  0.615\n",
      "2018-11-23 19:05:37  Training  762/1000  Loss  0.619\n",
      "2018-11-23 19:06:07  Training  763/1000  Loss  0.625\n",
      "2018-11-23 19:06:23  Training  764/1000  Loss  0.615\n",
      "2018-11-23 19:06:38  Training  765/1000  Loss  0.625\n",
      "2018-11-23 19:06:53  Training  766/1000  Loss  0.612\n",
      "2018-11-23 19:07:09  Training  767/1000  Loss  0.641\n",
      "2018-11-23 19:07:24  Training  768/1000  Loss  0.625\n",
      "2018-11-23 19:07:38  Training  769/1000  Loss  0.625\n",
      "2018-11-23 19:07:52  Training  770/1000  Loss  0.625\n",
      "2018-11-23 19:08:12  Training  771/1000  Loss  0.627\n",
      "2018-11-23 19:08:27  Training  772/1000  Loss  0.619\n",
      "2018-11-23 19:08:43  Training  773/1000  Loss  0.634\n",
      "2018-11-23 19:08:57  Training  774/1000  Loss  0.635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-23 19:09:15  Training  775/1000  Loss  0.618\n",
      "2018-11-23 19:09:38  Training  776/1000  Loss  0.615\n",
      "2018-11-23 19:09:54  Training  777/1000  Loss  0.608\n",
      "2018-11-23 19:10:14  Training  778/1000  Loss  0.597\n",
      "2018-11-23 19:10:28  Training  779/1000  Loss  0.627\n",
      "2018-11-23 19:10:43  Training  780/1000  Loss  0.612\n",
      "2018-11-23 19:11:03  Training  781/1000  Loss  0.604\n",
      "2018-11-23 19:11:17  Training  782/1000  Loss  0.616\n",
      "2018-11-23 19:11:32  Training  783/1000  Loss  0.610\n",
      "2018-11-23 19:11:47  Training  784/1000  Loss  0.621\n",
      "2018-11-23 19:12:05  Training  785/1000  Loss  0.625\n",
      "2018-11-23 19:12:19  Training  786/1000  Loss  0.639\n",
      "2018-11-23 19:12:33  Training  787/1000  Loss  0.624\n",
      "2018-11-23 19:12:49  Training  788/1000  Loss  0.612\n",
      "2018-11-23 19:13:04  Training  789/1000  Loss  0.603\n",
      "2018-11-23 19:13:20  Training  790/1000  Loss  0.616\n",
      "2018-11-23 19:13:33  Training  791/1000  Loss  0.631\n",
      "2018-11-23 19:13:48  Training  792/1000  Loss  0.622\n",
      "2018-11-23 19:14:09  Training  793/1000  Loss  0.633\n",
      "2018-11-23 19:14:30  Training  794/1000  Loss  0.631\n",
      "2018-11-23 19:14:47  Training  795/1000  Loss  0.621\n",
      "2018-11-23 19:15:01  Training  796/1000  Loss  0.631\n",
      "2018-11-23 19:15:17  Training  797/1000  Loss  0.622\n",
      "2018-11-23 19:15:32  Training  798/1000  Loss  0.626\n",
      "2018-11-23 19:15:47  Training  799/1000  Loss  0.626\n",
      "2018-11-23 19:16:02  Training  800/1000  Loss  0.621\n",
      "2018-11-23 19:16:17  Training  801/1000  Loss  0.654\n",
      "2018-11-23 19:16:31  Training  802/1000  Loss  0.668\n",
      "2018-11-23 19:16:46  Training  803/1000  Loss  0.670\n",
      "2018-11-23 19:17:02  Training  804/1000  Loss  0.680\n",
      "2018-11-23 19:17:16  Training  805/1000  Loss  0.670\n",
      "2018-11-23 19:17:30  Training  806/1000  Loss  0.667\n",
      "2018-11-23 19:17:46  Training  807/1000  Loss  0.667\n",
      "2018-11-23 19:18:04  Training  808/1000  Loss  0.647\n",
      "2018-11-23 19:18:19  Training  809/1000  Loss  0.650\n",
      "2018-11-23 19:18:33  Training  810/1000  Loss  0.640\n",
      "2018-11-23 19:18:48  Training  811/1000  Loss  0.642\n",
      "2018-11-23 19:19:03  Training  812/1000  Loss  0.647\n",
      "2018-11-23 19:19:17  Training  813/1000  Loss  0.636\n",
      "2018-11-23 19:19:36  Training  814/1000  Loss  0.643\n",
      "2018-11-23 19:19:53  Training  815/1000  Loss  0.634\n",
      "2018-11-23 19:20:08  Training  816/1000  Loss  0.638\n",
      "2018-11-23 19:20:25  Training  817/1000  Loss  0.635\n",
      "2018-11-23 19:20:39  Training  818/1000  Loss  0.654\n",
      "2018-11-23 19:20:53  Training  819/1000  Loss  0.647\n",
      "2018-11-23 19:21:08  Training  820/1000  Loss  0.653\n",
      "2018-11-23 19:21:26  Training  821/1000  Loss  0.644\n",
      "2018-11-23 19:21:58  Training  822/1000  Loss  0.640\n",
      "2018-11-23 19:22:16  Training  823/1000  Loss  0.625\n",
      "2018-11-23 19:22:31  Training  824/1000  Loss  0.621\n",
      "2018-11-23 19:22:49  Training  825/1000  Loss  0.624\n",
      "2018-11-23 19:23:04  Training  826/1000  Loss  0.618\n",
      "2018-11-23 19:23:22  Training  827/1000  Loss  0.625\n",
      "2018-11-23 19:23:38  Training  828/1000  Loss  0.635\n",
      "2018-11-23 19:23:52  Training  829/1000  Loss  0.632\n",
      "2018-11-23 19:24:07  Training  830/1000  Loss  0.621\n",
      "2018-11-23 19:24:27  Training  831/1000  Loss  0.606\n",
      "2018-11-23 19:24:42  Training  832/1000  Loss  0.615\n",
      "2018-11-23 19:24:57  Training  833/1000  Loss  0.610\n",
      "2018-11-23 19:25:12  Training  834/1000  Loss  0.614\n",
      "2018-11-23 19:25:27  Training  835/1000  Loss  0.618\n",
      "2018-11-23 19:25:42  Training  836/1000  Loss  0.631\n",
      "2018-11-23 19:25:56  Training  837/1000  Loss  0.625\n",
      "2018-11-23 19:26:12  Training  838/1000  Loss  0.618\n",
      "2018-11-23 19:26:26  Training  839/1000  Loss  0.627\n",
      "2018-11-23 19:26:39  Training  840/1000  Loss  0.620\n",
      "2018-11-23 19:26:58  Training  841/1000  Loss  0.631\n",
      "2018-11-23 19:27:12  Training  842/1000  Loss  0.618\n",
      "2018-11-23 19:27:26  Training  843/1000  Loss  0.625\n",
      "2018-11-23 19:27:40  Training  844/1000  Loss  0.629\n",
      "2018-11-23 19:27:56  Training  845/1000  Loss  0.644\n",
      "2018-11-23 19:28:12  Training  846/1000  Loss  0.646\n",
      "2018-11-23 19:28:26  Training  847/1000  Loss  0.640\n",
      "2018-11-23 19:28:42  Training  848/1000  Loss  0.646\n",
      "2018-11-23 19:28:56  Training  849/1000  Loss  0.646\n",
      "2018-11-23 19:29:09  Training  850/1000  Loss  0.638\n",
      "2018-11-23 19:29:24  Training  851/1000  Loss  0.641\n",
      "2018-11-23 19:29:39  Training  852/1000  Loss  0.626\n",
      "2018-11-23 19:29:55  Training  853/1000  Loss  0.625\n",
      "2018-11-23 19:30:19  Training  854/1000  Loss  0.644\n",
      "2018-11-23 19:30:37  Training  855/1000  Loss  0.635\n",
      "2018-11-23 19:30:53  Training  856/1000  Loss  0.624\n",
      "2018-11-23 19:31:07  Training  857/1000  Loss  0.634\n",
      "2018-11-23 19:31:21  Training  858/1000  Loss  0.635\n",
      "2018-11-23 19:31:34  Training  859/1000  Loss  0.637\n",
      "2018-11-23 19:31:51  Training  860/1000  Loss  0.619\n",
      "2018-11-23 19:32:07  Training  861/1000  Loss  0.608\n",
      "2018-11-23 19:32:21  Training  862/1000  Loss  0.609\n",
      "2018-11-23 19:32:35  Training  863/1000  Loss  0.603\n",
      "2018-11-23 19:32:49  Training  864/1000  Loss  0.613\n",
      "2018-11-23 19:33:04  Training  865/1000  Loss  0.608\n",
      "2018-11-23 19:33:16  Training  866/1000  Loss  0.603\n",
      "2018-11-23 19:33:30  Training  867/1000  Loss  0.596\n",
      "2018-11-23 19:33:43  Training  868/1000  Loss  0.603\n",
      "2018-11-23 19:33:57  Training  869/1000  Loss  0.616\n",
      "2018-11-23 19:34:13  Training  870/1000  Loss  0.609\n",
      "2018-11-23 19:34:29  Training  871/1000  Loss  0.641\n",
      "2018-11-23 19:34:49  Training  872/1000  Loss  0.631\n",
      "2018-11-23 19:35:03  Training  873/1000  Loss  0.628\n",
      "2018-11-23 19:35:20  Training  874/1000  Loss  0.631\n",
      "2018-11-23 19:35:33  Training  875/1000  Loss  0.627\n",
      "2018-11-23 19:35:47  Training  876/1000  Loss  0.627\n",
      "2018-11-23 19:36:18  Training  877/1000  Loss  0.616\n",
      "2018-11-23 19:36:36  Training  878/1000  Loss  0.624\n",
      "2018-11-23 19:36:53  Training  879/1000  Loss  0.643\n",
      "2018-11-23 19:37:07  Training  880/1000  Loss  0.649\n",
      "2018-11-23 19:37:24  Training  881/1000  Loss  0.634\n",
      "2018-11-23 19:37:37  Training  882/1000  Loss  0.644\n",
      "2018-11-23 19:37:52  Training  883/1000  Loss  0.641\n",
      "2018-11-23 19:38:07  Training  884/1000  Loss  0.626\n",
      "2018-11-23 19:38:23  Training  885/1000  Loss  0.609\n",
      "2018-11-23 19:38:38  Training  886/1000  Loss  0.605\n",
      "2018-11-23 19:38:53  Training  887/1000  Loss  0.605\n",
      "2018-11-23 19:39:07  Training  888/1000  Loss  0.614\n",
      "2018-11-23 19:39:22  Training  889/1000  Loss  0.611\n",
      "2018-11-23 19:39:38  Training  890/1000  Loss  0.618\n",
      "2018-11-23 19:39:55  Training  891/1000  Loss  0.613\n",
      "2018-11-23 19:40:12  Training  892/1000  Loss  0.604\n",
      "2018-11-23 19:40:26  Training  893/1000  Loss  0.589\n",
      "2018-11-23 19:40:39  Training  894/1000  Loss  0.597\n",
      "2018-11-23 19:40:54  Training  895/1000  Loss  0.596\n",
      "2018-11-23 19:41:08  Training  896/1000  Loss  0.593\n",
      "2018-11-23 19:41:23  Training  897/1000  Loss  0.602\n",
      "2018-11-23 19:41:36  Training  898/1000  Loss  0.608\n",
      "2018-11-23 19:41:50  Training  899/1000  Loss  0.592\n",
      "2018-11-23 19:42:05  Training  900/1000  Loss  0.598\n",
      "2018-11-23 19:42:20  Training  901/1000  Loss  0.593\n",
      "2018-11-23 19:42:33  Training  902/1000  Loss  0.592\n",
      "2018-11-23 19:42:47  Training  903/1000  Loss  0.591\n",
      "2018-11-23 19:43:01  Training  904/1000  Loss  0.602\n",
      "2018-11-23 19:43:15  Training  905/1000  Loss  0.600\n",
      "2018-11-23 19:43:31  Training  906/1000  Loss  0.590\n",
      "2018-11-23 19:43:44  Training  907/1000  Loss  0.592\n",
      "2018-11-23 19:44:04  Training  908/1000  Loss  0.596\n",
      "2018-11-23 19:44:19  Training  909/1000  Loss  0.593\n",
      "2018-11-23 19:44:35  Training  910/1000  Loss  0.587\n",
      "2018-11-23 19:44:53  Training  911/1000  Loss  0.593\n",
      "2018-11-23 19:45:07  Training  912/1000  Loss  0.593\n",
      "2018-11-23 19:45:21  Training  913/1000  Loss  0.597\n",
      "2018-11-23 19:45:39  Training  914/1000  Loss  0.598\n",
      "2018-11-23 19:45:54  Training  915/1000  Loss  0.616\n",
      "2018-11-23 19:46:11  Training  916/1000  Loss  0.616\n",
      "2018-11-23 19:46:25  Training  917/1000  Loss  0.607\n",
      "2018-11-23 19:46:47  Training  918/1000  Loss  0.604\n",
      "2018-11-23 19:47:07  Training  919/1000  Loss  0.603\n",
      "2018-11-23 19:47:23  Training  920/1000  Loss  0.605\n",
      "2018-11-23 19:47:39  Training  921/1000  Loss  0.611\n",
      "2018-11-23 19:47:53  Training  922/1000  Loss  0.611\n",
      "2018-11-23 19:48:09  Training  923/1000  Loss  0.631\n",
      "2018-11-23 19:48:23  Training  924/1000  Loss  0.622\n",
      "2018-11-23 19:48:37  Training  925/1000  Loss  0.631\n",
      "2018-11-23 19:48:51  Training  926/1000  Loss  0.620\n",
      "2018-11-23 19:49:06  Training  927/1000  Loss  0.623\n",
      "2018-11-23 19:49:20  Training  928/1000  Loss  0.638\n",
      "2018-11-23 19:49:39  Training  929/1000  Loss  0.640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-23 19:49:54  Training  930/1000  Loss  0.637\n",
      "2018-11-23 19:50:13  Training  931/1000  Loss  0.633\n",
      "2018-11-23 19:50:38  Training  932/1000  Loss  0.625\n",
      "2018-11-23 19:50:56  Training  933/1000  Loss  0.626\n",
      "2018-11-23 19:51:11  Training  934/1000  Loss  0.618\n",
      "2018-11-23 19:51:27  Training  935/1000  Loss  0.621\n",
      "2018-11-23 19:51:41  Training  936/1000  Loss  0.624\n",
      "2018-11-23 19:51:55  Training  937/1000  Loss  0.612\n",
      "2018-11-23 19:52:11  Training  938/1000  Loss  0.600\n",
      "2018-11-23 19:52:24  Training  939/1000  Loss  0.595\n",
      "2018-11-23 19:52:39  Training  940/1000  Loss  0.603\n",
      "2018-11-23 19:52:53  Training  941/1000  Loss  0.591\n",
      "2018-11-23 19:53:08  Training  942/1000  Loss  0.588\n",
      "2018-11-23 19:53:22  Training  943/1000  Loss  0.572\n",
      "2018-11-23 19:53:35  Training  944/1000  Loss  0.570\n",
      "2018-11-23 19:53:49  Training  945/1000  Loss  0.562\n",
      "2018-11-23 19:54:04  Training  946/1000  Loss  0.572\n",
      "2018-11-23 19:54:26  Training  947/1000  Loss  0.561\n",
      "2018-11-23 19:54:40  Training  948/1000  Loss  0.566\n",
      "2018-11-23 19:54:54  Training  949/1000  Loss  0.575\n",
      "2018-11-23 19:55:10  Training  950/1000  Loss  0.578\n",
      "2018-11-23 19:55:24  Training  951/1000  Loss  0.577\n",
      "2018-11-23 19:55:39  Training  952/1000  Loss  0.572\n",
      "2018-11-23 19:55:53  Training  953/1000  Loss  0.574\n",
      "2018-11-23 19:56:12  Training  954/1000  Loss  0.584\n",
      "2018-11-23 19:56:26  Training  955/1000  Loss  0.584\n",
      "2018-11-23 19:56:40  Training  956/1000  Loss  0.578\n",
      "2018-11-23 19:56:54  Training  957/1000  Loss  0.588\n",
      "2018-11-23 19:57:08  Training  958/1000  Loss  0.588\n",
      "2018-11-23 19:57:23  Training  959/1000  Loss  0.590\n",
      "2018-11-23 19:57:36  Training  960/1000  Loss  0.577\n",
      "2018-11-23 19:57:50  Training  961/1000  Loss  0.585\n",
      "2018-11-23 19:58:05  Training  962/1000  Loss  0.635\n",
      "2018-11-23 19:58:20  Training  963/1000  Loss  0.624\n",
      "2018-11-23 19:58:36  Training  964/1000  Loss  0.640\n",
      "2018-11-23 19:58:50  Training  965/1000  Loss  0.617\n",
      "2018-11-23 19:59:04  Training  966/1000  Loss  0.620\n",
      "2018-11-23 19:59:22  Training  967/1000  Loss  0.611\n",
      "2018-11-23 19:59:37  Training  968/1000  Loss  0.607\n",
      "2018-11-23 19:59:52  Training  969/1000  Loss  0.609\n",
      "2018-11-23 20:00:09  Training  970/1000  Loss  0.591\n",
      "2018-11-23 20:00:23  Training  971/1000  Loss  0.599\n",
      "2018-11-23 20:00:39  Training  972/1000  Loss  0.608\n",
      "2018-11-23 20:00:55  Training  973/1000  Loss  0.624\n",
      "2018-11-23 20:01:10  Training  974/1000  Loss  0.628\n",
      "2018-11-23 20:01:25  Training  975/1000  Loss  0.635\n",
      "2018-11-23 20:01:40  Training  976/1000  Loss  0.633\n",
      "2018-11-23 20:01:55  Training  977/1000  Loss  0.634\n",
      "2018-11-23 20:02:11  Training  978/1000  Loss  0.614\n",
      "2018-11-23 20:02:26  Training  979/1000  Loss  0.641\n",
      "2018-11-23 20:02:40  Training  980/1000  Loss  0.637\n",
      "2018-11-23 20:02:54  Training  981/1000  Loss  0.631\n",
      "2018-11-23 20:03:10  Training  982/1000  Loss  0.634\n",
      "2018-11-23 20:03:25  Training  983/1000  Loss  0.641\n",
      "2018-11-23 20:03:42  Training  984/1000  Loss  0.618\n",
      "2018-11-23 20:03:56  Training  985/1000  Loss  0.623\n",
      "2018-11-23 20:04:12  Training  986/1000  Loss  0.628\n",
      "2018-11-23 20:04:30  Training  987/1000  Loss  0.613\n",
      "2018-11-23 20:04:45  Training  988/1000  Loss  0.625\n",
      "2018-11-23 20:04:59  Training  989/1000  Loss  0.624\n",
      "2018-11-23 20:05:13  Training  990/1000  Loss  0.624\n",
      "2018-11-23 20:05:29  Training  991/1000  Loss  0.619\n",
      "2018-11-23 20:05:44  Training  992/1000  Loss  0.605\n",
      "2018-11-23 20:06:12  Training  993/1000  Loss  0.594\n",
      "2018-11-23 20:06:31  Training  994/1000  Loss  0.617\n",
      "2018-11-23 20:06:48  Training  995/1000  Loss  0.619\n",
      "2018-11-23 20:07:03  Training  996/1000  Loss  0.613\n",
      "2018-11-23 20:07:17  Training  997/1000  Loss  0.631\n",
      "2018-11-23 20:07:32  Training  998/1000  Loss  0.613\n",
      "2018-11-23 20:07:49  Training  999/1000  Loss  0.626\n",
      "2018-11-23 20:08:13  Training 1000/1000  Loss  0.618\n",
      "Predicting  1/22\n",
      "Predicting 22/22\n",
      "Predicting  1/22\n",
      "Predicting 22/22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/numpy/core/fromnumeric.py:51: FutureWarning: 'argmax' is deprecated, use 'idxmax' instead. The behavior of 'argmax'\n",
      "will be corrected to return the positional maximum in the future.\n",
      "Use 'series.values.argmax' to get the position of the maximum now.\n",
      "  return getattr(obj, method)(*args, **kwds)\n",
      "/anaconda2/lib/python2.7/site-packages/turicreate/toolkits/object_detector/_evaluation.py:56: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  if best_iou > iou_threshold and not gts.ix[best_gt_index, 'correct_%d' % th_index]:\n",
      "/anaconda2/lib/python2.7/site-packages/turicreate/toolkits/object_detector/_evaluation.py:57: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  gts.ix[best_gt_index, 'correct_%d' % th_index] = True\n",
      "WARNING:root:Keras version 2.2.4 detected. Last version known to be fully compatible of Keras is 2.1.3 .\n"
     ]
    }
   ],
   "source": [
    "# Create a model\n",
    "model = tc.object_detector.create(train_data,feature='image')\n",
    "\n",
    "# Save predictions to an SArray\n",
    "predictions = model.predict(test_data)\n",
    "\n",
    "# Evaluate the model and save the results into a dictionary\n",
    "metrics = model.evaluate(test_data)\n",
    "\n",
    "# Save the model for later use in Turi Create\n",
    "model.save('my_model.model')\n",
    "\n",
    "# Export for use in Core ML\n",
    "model.export_coreml('My_CustomObjectDetector.mlmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>Materializing SFrame...</pre>"
      ],
      "text/plain": [
       "Materializing SFrame..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Done.</pre>"
      ],
      "text/plain": [
       "Done."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_data['predictions'] = predictions\n",
    "test_data['image_with_predictions'] = \\\n",
    "    tc.object_detector.util.draw_bounding_boxes(test_data['image'], test_data['predictions'])\n",
    "#test_data[['image', 'image_with_predictions']].explore()\n",
    "test_data.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
